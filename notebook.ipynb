{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Seq2Seq Translation with Attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "<span style=\"font-size:1.15em;\">In this notebook, we'll be exploring the use of a sequence-to-sequence (Seq2Seq) model for machine translation. Our model is based on a neural network architecture that uses an encoder to process the input text and a decoder to generate the translated output. Specifically, we'll be training our model to translate text from one language to another, using an Arabic-to-English translation task as an example. Let's dive in and see how our model performs!</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T21:41:01.999098Z",
     "iopub.status.busy": "2023-05-06T21:41:01.998421Z",
     "iopub.status.idle": "2023-05-06T21:41:02.008067Z",
     "shell.execute_reply": "2023-05-06T21:41:02.007015Z",
     "shell.execute_reply.started": "2023-05-06T21:41:01.999063Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-05-06T21:41:02.013429Z",
     "iopub.status.busy": "2023-05-06T21:41:02.013098Z",
     "iopub.status.idle": "2023-05-06T21:41:05.566259Z",
     "shell.execute_reply": "2023-05-06T21:41:05.565244Z",
     "shell.execute_reply.started": "2023-05-06T21:41:02.013402Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import unicodedata\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pyarabic.araby as araby\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T21:41:05.568870Z",
     "iopub.status.busy": "2023-05-06T21:41:05.568144Z",
     "iopub.status.idle": "2023-05-06T21:41:05.619898Z",
     "shell.execute_reply": "2023-05-06T21:41:05.618005Z",
     "shell.execute_reply.started": "2023-05-06T21:41:05.568820Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "# %%script echo skipping\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Download and preprocess our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T21:41:05.623785Z",
     "iopub.status.busy": "2023-05-06T21:41:05.623446Z",
     "iopub.status.idle": "2023-05-06T21:41:05.742817Z",
     "shell.execute_reply": "2023-05-06T21:41:05.741604Z",
     "shell.execute_reply.started": "2023-05-06T21:41:05.623758Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "nltk.download('punkt')\n",
    "def tokenize_ar(text):\n",
    "    return [tok for tok in nltk.tokenize.wordpunct_tokenize(unicodeToAscii(text))]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok for tok in nltk.tokenize.wordpunct_tokenize(unicodeToAscii(text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T21:41:05.746285Z",
     "iopub.status.busy": "2023-05-06T21:41:05.745844Z",
     "iopub.status.idle": "2023-05-06T21:41:23.584788Z",
     "shell.execute_reply": "2023-05-06T21:41:23.583680Z",
     "shell.execute_reply.started": "2023-05-06T21:41:05.746248Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install datasets\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset('opus100', 'ar-en')\n",
    "\n",
    "df=dataset['train']['translation']\n",
    "df = pd.DataFrame(df)\n",
    "df = df.head(100000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<span style=\"font-size:1.1em;\">The 'opus100' dataset is a parallel corpus containing Arabic-English sentence pairs. It can be loaded using the Hugging Face library's 'load_dataset' function.</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T21:41:23.587374Z",
     "iopub.status.busy": "2023-05-06T21:41:23.586953Z",
     "iopub.status.idle": "2023-05-06T21:41:23.601854Z",
     "shell.execute_reply": "2023-05-06T21:41:23.600195Z",
     "shell.execute_reply.started": "2023-05-06T21:41:23.587329Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tashkeel(text):\n",
    "    text = araby.strip_diacritics(text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def clean(text):\n",
    "    return re.sub(r'[_|\\d+|\\\\|\\-|؛|،|,|\\[|\\]|\\(|\\)|\\\"|/|%|!|,|.|:|♪|«|»|123456789]', '', text)\n",
    "\n",
    "abbr_dict = {\n",
    "    \"what's\": \"what is\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who're\": \"who are\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where're\": \"where are\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when're\": \"when are\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"how're\": \"how are\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"there're\": \"there are\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"not've\": \"not have\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"can't\": \"can not\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"here's\":\"here is\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"ain't\": \"am not\",\n",
    "    \"dont\": \"do not\",\n",
    "    \"havent\":\"have not\",\n",
    "    \"cant\":\"can not\",\n",
    "    \"cannot\":\"can not\",\n",
    "    \"wouldnt\":\"would not\",\n",
    "    \"hasnt\":\"has not\",\n",
    "    \"hadnt\":\"had not\",\n",
    "    \"doesnt\": \"does not\",\n",
    "    \"didnt\": \"did not\",\n",
    "    \"wasnt\": \"was not\",\n",
    "    \"werent\": \"were not\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"I'd've\": \"I would have\",\n",
    "    \"I'll\": \"I will\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "def replace_abbreviations(text):\n",
    "    text = re.sub('’', '\\'', text)\n",
    "    text = re.sub(r'\\bdon t\\b', 'do not', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\b[mf](\\d+)\\b', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\b(\\d+)[mf]\\b', '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    for word in text.split():\n",
    "        if word.lower() in abbr_dict:\n",
    "            text = re.sub(r'\\b{}\\b'.format(word), abbr_dict[word.lower()], text, flags=re.IGNORECASE)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T21:41:23.604454Z",
     "iopub.status.busy": "2023-05-06T21:41:23.603515Z",
     "iopub.status.idle": "2023-05-06T21:41:26.526232Z",
     "shell.execute_reply": "2023-05-06T21:41:26.525205Z",
     "shell.execute_reply.started": "2023-05-06T21:41:23.604415Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:00<00:00, 131893.82it/s]\n",
      "100%|██████████| 100000/100000 [00:00<00:00, 299897.32it/s]\n",
      "100%|██████████| 100000/100000 [00:00<00:00, 303919.88it/s]\n",
      "100%|██████████| 100000/100000 [00:01<00:00, 71711.86it/s]\n"
     ]
    }
   ],
   "source": [
    "df['en'] = df['en'].str.lower()\n",
    "df['ar'] = df['ar'].progress_apply(tashkeel)\n",
    "df['ar'] = df['ar'].progress_apply(clean)\n",
    "df['en'] = df['en'].progress_apply(clean)\n",
    "df['en'] = df['en'].progress_apply(replace_abbreviations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T21:41:26.528744Z",
     "iopub.status.busy": "2023-05-06T21:41:26.527762Z",
     "iopub.status.idle": "2023-05-06T21:41:26.673492Z",
     "shell.execute_reply": "2023-05-06T21:41:26.672465Z",
     "shell.execute_reply.started": "2023-05-06T21:41:26.528701Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ar</th>\n",
       "      <th>en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>مقرف</td>\n",
       "      <td>ugh disgusting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>لا أحب ذلك</td>\n",
       "      <td>i do not like it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>هل حصلت على جزء ?</td>\n",
       "      <td>did you get the part?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>إتركه</td>\n",
       "      <td>leave him</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>هذا ليس من شأنك</td>\n",
       "      <td>it is none of your business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ar                            en\n",
       "5               مقرف                ugh disgusting\n",
       "6         لا أحب ذلك              i do not like it\n",
       "7  هل حصلت على جزء ?         did you get the part?\n",
       "8              إتركه                     leave him\n",
       "9    هذا ليس من شأنك   it is none of your business"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T21:41:26.675471Z",
     "iopub.status.busy": "2023-05-06T21:41:26.675078Z",
     "iopub.status.idle": "2023-05-06T21:41:26.723386Z",
     "shell.execute_reply": "2023-05-06T21:41:26.722359Z",
     "shell.execute_reply.started": "2023-05-06T21:41:26.675424Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90000, 2)\n",
      "(5000, 2)\n",
      "(5000, 2)\n"
     ]
    }
   ],
   "source": [
    "train, val, test = np.split(df.sample(frac=1, random_state=42), \n",
    "                                [int(.9*len(df)), int(.95*len(df))])\n",
    "\n",
    "print(train.shape, val.shape, test.shape, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T21:41:26.725236Z",
     "iopub.status.busy": "2023-05-06T21:41:26.724904Z",
     "iopub.status.idle": "2023-05-06T21:41:31.090307Z",
     "shell.execute_reply": "2023-05-06T21:41:31.089197Z",
     "shell.execute_reply.started": "2023-05-06T21:41:26.725205Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter, src=True):\n",
    "    for text in data_iter:\n",
    "        if src:\n",
    "            yield tokenize_ar(text)\n",
    "        else:\n",
    "            yield tokenize_en(text)\n",
    "\n",
    "src_vocab = build_vocab_from_iterator(yield_tokens(iter(train['ar'])),\n",
    "                                      min_freq=2, \n",
    "                                      specials=[\"\"])\n",
    "src_vocab.set_default_index(src_vocab[\"\"])\n",
    "\n",
    "trg_vocab = build_vocab_from_iterator(yield_tokens(iter(train['en']),src=False), \n",
    "                                      min_freq=2, \n",
    "                                      specials=[\"\"])\n",
    "trg_vocab.set_default_index(trg_vocab[\"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T21:41:31.095348Z",
     "iopub.status.busy": "2023-05-06T21:41:31.095026Z",
     "iopub.status.idle": "2023-05-06T21:41:31.101094Z",
     "shell.execute_reply": "2023-05-06T21:41:31.100026Z",
     "shell.execute_reply.started": "2023-05-06T21:41:31.095321Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37705\n",
      "18343\n"
     ]
    }
   ],
   "source": [
    "print(len(src_vocab))\n",
    "print(len(trg_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T21:41:31.104337Z",
     "iopub.status.busy": "2023-05-06T21:41:31.103415Z",
     "iopub.status.idle": "2023-05-06T21:41:31.118289Z",
     "shell.execute_reply": "2023-05-06T21:41:31.117035Z",
     "shell.execute_reply.started": "2023-05-06T21:41:31.104295Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess(sequence, vocab, src=True):\n",
    "    if src:\n",
    "        tokens = tokenize_ar(sequence.lower())\n",
    "    else:\n",
    "        tokens = tokenize_en(sequence.lower())\n",
    "\n",
    "    sequence = []\n",
    "    sequence.append(vocab[''])\n",
    "    sequence.extend([vocab[token] for token in tokens])\n",
    "    sequence.append(vocab[''])\n",
    "    sequence = torch.Tensor(sequence)\n",
    "    return sequence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load our data into a DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T21:41:31.120237Z",
     "iopub.status.busy": "2023-05-06T21:41:31.119832Z",
     "iopub.status.idle": "2023-05-06T21:41:31.135911Z",
     "shell.execute_reply": "2023-05-06T21:41:31.133744Z",
     "shell.execute_reply.started": "2023-05-06T21:41:31.120203Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, src, trg, src_vocab, trg_vocab):\n",
    "        self.src_seqs = src\n",
    "        self.trg_seqs = trg\n",
    "        self.num_total_seqs = len(self.src_seqs)\n",
    "        self.src_vocab= src_vocab\n",
    "        self.trg_vocab = trg_vocab\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        src_seq = self.src_seqs.iloc[index]\n",
    "        trg_seq = self.trg_seqs.iloc[index]\n",
    "        src_seq = self.preprocess(src_seq, self.src_vocab)\n",
    "        trg_seq = self.preprocess(trg_seq, self.trg_vocab, src=False)\n",
    "        return src_seq, trg_seq\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_total_seqs\n",
    "\n",
    "    def preprocess(self, sequence, vocab, src=True):\n",
    "        if src:\n",
    "            tokens = tokenize_ar(sequence.lower())\n",
    "        else:\n",
    "            tokens = tokenize_en(sequence.lower())\n",
    "\n",
    "        sequence = []\n",
    "        sequence.append(vocab[''])\n",
    "        sequence.extend([vocab[token] for token in tokens])\n",
    "        sequence.append(vocab[''])\n",
    "        sequence = torch.Tensor(sequence)\n",
    "        return sequence\n",
    "\n",
    "\n",
    "def collate_fn(data):\n",
    "    def merge(sequences):\n",
    "        lengths = [len(seq) for seq in sequences]\n",
    "        padded_seqs = torch.zeros(len(sequences), max(lengths)).long()\n",
    "        for i, seq in enumerate(sequences):\n",
    "            end = lengths[i]\n",
    "            padded_seqs[i, :end] = seq[:end]\n",
    "        return padded_seqs, lengths\n",
    "\n",
    "    data.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    src_seqs, trg_seqs = zip(*data)\n",
    "    src_seqs, src_lengths = merge(src_seqs)\n",
    "    trg_seqs, trg_lengths = merge(trg_seqs)\n",
    "\n",
    "    return src_seqs, src_lengths, trg_seqs, trg_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T21:41:31.139220Z",
     "iopub.status.busy": "2023-05-06T21:41:31.138430Z",
     "iopub.status.idle": "2023-05-06T21:41:31.148592Z",
     "shell.execute_reply": "2023-05-06T21:41:31.147236Z",
     "shell.execute_reply.started": "2023-05-06T21:41:31.139180Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_loader(src, trg, src_vocab, trg_vocab, batch_size=128):\n",
    "    dataset = CustomDataset(src, trg, src_vocab, trg_vocab)\n",
    "    data_loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=True,\n",
    "                                              collate_fn=collate_fn)\n",
    "\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T21:41:31.151160Z",
     "iopub.status.busy": "2023-05-06T21:41:31.150213Z",
     "iopub.status.idle": "2023-05-06T21:41:31.159825Z",
     "shell.execute_reply": "2023-05-06T21:41:31.158883Z",
     "shell.execute_reply.started": "2023-05-06T21:41:31.151112Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T21:41:31.162182Z",
     "iopub.status.busy": "2023-05-06T21:41:31.161251Z",
     "iopub.status.idle": "2023-05-06T21:41:31.170925Z",
     "shell.execute_reply": "2023-05-06T21:41:31.170093Z",
     "shell.execute_reply.started": "2023-05-06T21:41:31.162139Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_loader = get_loader(train['ar'], train['en'], src_vocab, trg_vocab, batch_size=BATCH_SIZE)\n",
    "val_loader = get_loader(val['ar'], val['en'], src_vocab, trg_vocab, batch_size=BATCH_SIZE)\n",
    "test_loader = get_loader(test['ar'], test['en'], src_vocab, trg_vocab, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Create our Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T21:41:31.173177Z",
     "iopub.status.busy": "2023-05-06T21:41:31.172217Z",
     "iopub.status.idle": "2023-05-06T21:41:31.184994Z",
     "shell.execute_reply": "2023-05-06T21:41:31.183970Z",
     "shell.execute_reply.started": "2023-05-06T21:41:31.173149Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
    "        return outputs, hidden"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Implementing Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T21:41:31.188943Z",
     "iopub.status.busy": "2023-05-06T21:41:31.188479Z",
     "iopub.status.idle": "2023-05-06T21:41:31.198650Z",
     "shell.execute_reply": "2023-05-06T21:41:31.197331Z",
     "shell.execute_reply.started": "2023-05-06T21:41:31.188906Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        return F.softmax(attention, dim=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T21:41:31.201416Z",
     "iopub.status.busy": "2023-05-06T21:41:31.200431Z",
     "iopub.status.idle": "2023-05-06T21:41:31.212659Z",
     "shell.execute_reply": "2023-05-06T21:41:31.211524Z",
     "shell.execute_reply.started": "2023-05-06T21:41:31.201379Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
    "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        a = self.attention(hidden, encoder_outputs)\n",
    "        a = a.unsqueeze(1)\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        \n",
    "        assert (output == hidden).all()\n",
    "        \n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        \n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
    "        return prediction, hidden.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T21:41:31.216872Z",
     "iopub.status.busy": "2023-05-06T21:41:31.216539Z",
     "iopub.status.idle": "2023-05-06T21:41:31.227495Z",
     "shell.execute_reply": "2023-05-06T21:41:31.226343Z",
     "shell.execute_reply.started": "2023-05-06T21:41:31.216832Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        \n",
    "        input = trg[0,:]\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[t] if teacher_force else top1\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T21:41:31.229923Z",
     "iopub.status.busy": "2023-05-06T21:41:31.229502Z",
     "iopub.status.idle": "2023-05-06T21:41:33.938609Z",
     "shell.execute_reply": "2023-05-06T21:41:33.937580Z",
     "shell.execute_reply.started": "2023-05-06T21:41:31.229850Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(src_vocab)\n",
    "OUTPUT_DIM = len(trg_vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "ENC_HID_DIM = 512\n",
    "DEC_HID_DIM = 512\n",
    "ENC_DROPOUT = 0.3\n",
    "DEC_DROPOUT = 0.3\n",
    "\n",
    "\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT).to(device)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn).to(device)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T21:41:33.940655Z",
     "iopub.status.busy": "2023-05-06T21:41:33.940255Z",
     "iopub.status.idle": "2023-05-06T21:41:33.953234Z",
     "shell.execute_reply": "2023-05-06T21:41:33.951639Z",
     "shell.execute_reply.started": "2023-05-06T21:41:33.940619Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(37705, 256)\n",
       "    (rnn): GRU(256, 512, bidirectional=True)\n",
       "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): Attention(\n",
       "      (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
       "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
       "    )\n",
       "    (embedding): Embedding(18343, 256)\n",
       "    (rnn): GRU(1280, 512)\n",
       "    (fc_out): Linear(in_features=1792, out_features=18343, bias=True)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "            \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<span style=\"font-size:1.1em;\">Our Seq2Seq model consists of an encoder that processes the input text using an embedding layer and a bidirectional GRU, followed by a fully connected layer with dropout. The decoder uses attention to focus on different parts of the input and generates the output text using an embedding layer, a unidirectional GRU, and a fully connected layer with dropout."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"general_scheme.png\" alt=\"general_scheme\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "[Voita, L. (2021). Sequence-to-Sequence Models with Attention.](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T21:41:33.956802Z",
     "iopub.status.busy": "2023-05-06T21:41:33.954945Z",
     "iopub.status.idle": "2023-05-06T21:41:33.963520Z",
     "shell.execute_reply": "2023-05-06T21:41:33.962406Z",
     "shell.execute_reply.started": "2023-05-06T21:41:33.956764Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 53,670,567 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T21:41:33.966376Z",
     "iopub.status.busy": "2023-05-06T21:41:33.965485Z",
     "iopub.status.idle": "2023-05-06T21:41:33.974834Z",
     "shell.execute_reply": "2023-05-06T21:41:33.974023Z",
     "shell.execute_reply.started": "2023-05-06T21:41:33.966337Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = trg_vocab['']).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T21:41:33.977095Z",
     "iopub.status.busy": "2023-05-06T21:41:33.976365Z",
     "iopub.status.idle": "2023-05-06T21:41:33.985229Z",
     "shell.execute_reply": "2023-05-06T21:41:33.984436Z",
     "shell.execute_reply.started": "2023-05-06T21:41:33.977060Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = torch.transpose(batch[0], 0,1).to(device)\n",
    "        trg = torch.transpose(batch[2], 0,1).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].reshape(-1)\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T21:41:33.987631Z",
     "iopub.status.busy": "2023-05-06T21:41:33.986851Z",
     "iopub.status.idle": "2023-05-06T21:41:33.998099Z",
     "shell.execute_reply": "2023-05-06T21:41:33.997123Z",
     "shell.execute_reply.started": "2023-05-06T21:41:33.987589Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = torch.transpose(batch[0], 0, 1).to(device)\n",
    "            trg = torch.transpose(batch[2], 0, 1).to(device)\n",
    "            output = model(src, trg, 0)\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].reshape(-1)\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T21:41:34.000593Z",
     "iopub.status.busy": "2023-05-06T21:41:33.999845Z",
     "iopub.status.idle": "2023-05-06T21:41:34.008679Z",
     "shell.execute_reply": "2023-05-06T21:41:34.007771Z",
     "shell.execute_reply.started": "2023-05-06T21:41:34.000554Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    time = end_time - start_time\n",
    "    mins = int(time / 60)\n",
    "    secs = int(time - (mins * 60))\n",
    "    return mins, secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T21:41:34.010997Z",
     "iopub.status.busy": "2023-05-06T21:41:34.010319Z",
     "iopub.status.idle": "2023-05-06T23:45:27.260693Z",
     "shell.execute_reply": "2023-05-06T23:45:27.259529Z",
     "shell.execute_reply.started": "2023-05-06T21:41:34.010962Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 21m 28s\n",
      "\tTrain Loss: 5.630 | Train PPL: 278.648\n",
      "\t Val. Loss: 5.393 |  Val. PPL: 219.891\n",
      "Epoch: 02 | Time: 21m 32s\n",
      "\tTrain Loss: 4.529 | Train PPL:  92.630\n",
      "\t Val. Loss: 5.107 |  Val. PPL: 165.123\n",
      "Epoch: 03 | Time: 20m 7s\n",
      "\tTrain Loss: 3.859 | Train PPL:  47.402\n",
      "\t Val. Loss: 5.102 |  Val. PPL: 164.297\n",
      "Epoch: 04 | Time: 20m 13s\n",
      "\tTrain Loss: 3.375 | Train PPL:  29.222\n",
      "\t Val. Loss: 5.262 |  Val. PPL: 192.843\n",
      "Epoch: 05 | Time: 20m 11s\n",
      "\tTrain Loss: 3.061 | Train PPL:  21.341\n",
      "\t Val. Loss: 5.385 |  Val. PPL: 218.133\n",
      "Epoch: 06 | Time: 20m 17s\n",
      "\tTrain Loss: 2.889 | Train PPL:  17.978\n",
      "\t Val. Loss: 5.506 |  Val. PPL: 246.208\n",
      "Early stopping after epoch 6: no improvement for 3 epochs.\n",
      "CPU times: user 1h 50min, sys: 13min 22s, total: 2h 3min 22s\n",
      "Wall time: 2h 3min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "N_EPOCHS = 25\n",
    "CLIP = 1\n",
    "PATIENCE = 3\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "early_stop_counter = 0\n",
    "best_valid_epoch = 0\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, val_loader, criterion)\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        best_valid_epoch = epoch\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "        early_stop_counter = 0\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= PATIENCE:\n",
    "            print(f'Early stopping after epoch {epoch+1}: no improvement for {PATIENCE} epochs.')\n",
    "            break\n",
    "\n",
    "if early_stop_counter < PATIENCE:\n",
    "    print(f'Best validation loss of {best_valid_loss:.3f} at epoch {best_valid_epoch+1}.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<span style=\"font-size:1.1em;\">Perplexity (PPL) is a commonly used evaluation metric in natural language processing tasks. It measures the uncertainty or perplexity of a language model's predictions on a given test set. The lower the perplexity, the better the model is at predicting the test data.</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T23:45:27.267128Z",
     "iopub.status.busy": "2023-05-06T23:45:27.266565Z",
     "iopub.status.idle": "2023-05-06T23:45:52.937909Z",
     "shell.execute_reply": "2023-05-06T23:45:52.936693Z",
     "shell.execute_reply.started": "2023-05-06T23:45:27.267093Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 5.118 | Test PPL: 167.046 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model.pt'))\n",
    "test_loss = evaluate(model, test_loader, criterion)\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## The Translation Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T23:45:52.939863Z",
     "iopub.status.busy": "2023-05-06T23:45:52.939499Z",
     "iopub.status.idle": "2023-05-06T23:45:52.945202Z",
     "shell.execute_reply": "2023-05-06T23:45:52.943922Z",
     "shell.execute_reply.started": "2023-05-06T23:45:52.939836Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "source = \"كيف حالك ؟\"\n",
    "input = preprocess(source, src_vocab)\n",
    "input = input[:,None].to(torch.int64).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T23:45:52.947682Z",
     "iopub.status.busy": "2023-05-06T23:45:52.946950Z",
     "iopub.status.idle": "2023-05-06T23:45:52.964097Z",
     "shell.execute_reply": "2023-05-06T23:45:52.962887Z",
     "shell.execute_reply.started": "2023-05-06T23:45:52.947637Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "target = torch.zeros(len(source.split(' '))+2,1,).to(torch.int64)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    input = input.to(device)\n",
    "    target = target.to(device)\n",
    "    output = model(input, target, 0)\n",
    "    output_dim = output.shape[-1]\n",
    "    output = output[1:].view(-1, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T23:45:52.967336Z",
     "iopub.status.busy": "2023-05-06T23:45:52.966563Z",
     "iopub.status.idle": "2023-05-06T23:45:52.973907Z",
     "shell.execute_reply": "2023-05-06T23:45:52.972747Z",
     "shell.execute_reply.started": "2023-05-06T23:45:52.967283Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "prediction = []\n",
    "for i in output:\n",
    "    prediction.append(torch.argmax(i).item())\n",
    "tokens = trg_vocab.lookup_tokens(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T23:45:52.976155Z",
     "iopub.status.busy": "2023-05-06T23:45:52.975672Z",
     "iopub.status.idle": "2023-05-06T23:45:52.990400Z",
     "shell.execute_reply": "2023-05-06T23:45:52.989252Z",
     "shell.execute_reply.started": "2023-05-06T23:45:52.976119Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how are you ?'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "TreebankWordDetokenizer().detokenize(tokens).replace('', \"\").replace('\"',\"\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
